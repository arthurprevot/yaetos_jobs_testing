# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  # Sample jobs added below. They can be deleted.
  examples/ex0_extraction_job.py:
    description: "Sample API extraction job, pulling public wikipedia data."
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{base_path}/raw/wiki/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    frequency: 1 day
    start_date: "{today}T07:00:00"
    spark_boot: False

  examples/ex1_sql_job.sql:
    description: "shows a sql job, running on top of Pandas."
    py_job: 'jobs/generic/sql_pandas_job.py'
    inputs:
      some_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/frontroom/wiki_ex1_pandas_sql/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [examples/ex0_extraction_job.py]
    spark_boot: False

  examples/ex1_pandas_api_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{base_path}/frontroom/wiki_ex1_pandas_api/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    dependencies: [examples/ex0_extraction_job.py]
    spark_boot: False

  examples/ex1_sql_spark_job:
    description: "shows a sql job, running on top of Spark."
    py_job: 'jobs/generic/sql_spark_job.py'
    sql_file: 'jobs/examples/ex1_sql_job.sql'
    inputs:
      some_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/frontroom/wiki_ex1_spark_sql/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    repartition: 1

  examples/ex1_spark_api_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/raw/wiki/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/frontroom/wiki_ex1_spark_api/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  # Your jobs should be added here.


# ----- Params -------
common_params:
  all_mode_params:
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.0' # options: '2.4' 'or '3.0'
  mode_specific_params:
    prod_EMR:
      base_path: s3://prod-spark  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      base_path: s3://dev-spark  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      base_path: ./data  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_redshift_push: False
      save_schemas: True
      manage_git_info: False
